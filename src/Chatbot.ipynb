{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375d0a33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Repo\\Codemedics\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import iris\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from chatbot_core.orchestrator import Orchestrator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c150491c",
   "metadata": {},
   "source": [
    "### Connect to server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11fbff3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "conn = iris.connect(\"localhost\", 32782, \"DEMO\", \"_SYSTEM\", \"ISCDEMO\") # Server, Port , Namespace, Username, Password\n",
    "cursor = conn.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99536ff2",
   "metadata": {},
   "source": [
    "### Make database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4269a401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to InterSystems IRIS\n"
     ]
    }
   ],
   "source": [
    "print(\"Connected to InterSystems IRIS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "792a1157",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create database\n",
    "df = pd.read_json(\"./data/data.json\") #pd.DataFrame(out, columns=cols)  #replace with Tristans/Iaroslavs code\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4d783a",
   "metadata": {},
   "source": [
    "### Make encoding / add it to database\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ecf9ea11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 5/5 [00:06<00:00,  1.36s/it]\n"
     ]
    }
   ],
   "source": [
    "#Make encoding \n",
    "#pip install sentence-transformers\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2') #might not work, if this is the case use instead ''\n",
    "\n",
    "\n",
    "def vectorize_content(df):\n",
    "    embeddings = model.encode(df['content'].tolist(), normalize_embeddings=True, show_progress_bar=True)\n",
    "    return embeddings\n",
    "\n",
    "def vectorize_filename(df):\n",
    "    embeddings = model.encode(df['filename'].tolist(), normalize_embeddings=True, show_progress_bar=True)\n",
    "    return embeddings\n",
    "\n",
    "embeddings = vectorize_content(df)\n",
    "df['vector'] = embeddings.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd0e3b4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "###Add to Iris:\n",
    "def create_table(df):\n",
    "    table_name = \"VectorSearch.ORGstruct\"\n",
    "\n",
    "    create_table_query = f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {table_name} (\n",
    "    id INTEGER,\n",
    "    filename LONGVARCHAR,\n",
    "    content LONGVARCHAR,\n",
    "    vector VECTOR(DOUBLE, 786)\n",
    "    )\n",
    "    \"\"\"\n",
    "\n",
    "    cursor.execute(create_table_query)\n",
    "\n",
    "\n",
    "\n",
    "    insert_query = f\"INSERT INTO {table_name} (id, filename, content, vector) values (?, ?, ?, TO_VECTOR(?))\"\n",
    "    df[\"vector\"] = df[\"vector\"].astype(str)\n",
    "\n",
    "    rows_list = df[[\"id\", \"filename\", \"content\", \"vector\"]].values.tolist()\n",
    "    cursor.executemany(insert_query, rows_list)\n",
    "    print(\"Done\")\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "create_table(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66124774",
   "metadata": {},
   "source": [
    "### LLM setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec4158ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "  api_key=\"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\"\n",
    ")\n",
    "OPENAI_API_KEY = \"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\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66246a7",
   "metadata": {},
   "source": [
    "### OpenAI SDK compatibility\n",
    "\n",
    "The python-openai package changed its API in recent major releases. If you see errors like `APIRemovedInV1` or `ChatCompletion.create` failures, you have two options:\n",
    "\n",
    "- Pin to the older client (backwards compatible): `pip install openai==0.28` — this keeps support for `openai.ChatCompletion.create`.\n",
    "- Prefer the new client API: create an `OpenAI` client and use the `client.chat.completions.create(...)` or `client.chat.create(...)` methods. The notebook and `agents` module handle both styles and also support injecting a custom `llm_client` for demos and testing.\n",
    "\n",
    "If you're running the notebook interactively, it's safest to set your `OPENAI_API_KEY` as an environment variable rather than hard-coding it in the notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16bbcd53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NotebookChatAdapter created as `adapter`. Use adapter.handle_query(<text>) or adapter.vector_search_iris(<text>)\n"
     ]
    }
   ],
   "source": [
    "# Adapter to use the new Orchestrator + keep IRIS / vector helpers\n",
    "# This replaces the old large RAGChatbot class with a small adapter that\n",
    "# delegates to chatbot_core.Orchestrator and still exposes the IRIS-based\n",
    "# vector search and the document helper functionality that the notebook\n",
    "# earlier implemented.\n",
    "\n",
    "class NotebookChatAdapter:\n",
    "    def __init__(self, orchestrator=None, conn=None, cursor=None, embedding_model=None):\n",
    "        # orchestrator (chatbot_core.orchestrator.Orchestrator) is preferred\n",
    "        self.orchestrator = orchestrator or Orchestrator()\n",
    "\n",
    "        # keep existing IRIS connection and DB cursor if available\n",
    "        self.conn = conn\n",
    "        self.cursor = cursor\n",
    "\n",
    "        # reuse notebook-level embedding model (variable `model`) if present\n",
    "        self.embedding_model = embedding_model or globals().get(\"model\") or SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "        # document state (mirrors earlier notebook behaviour)\n",
    "        self.current_doc = None\n",
    "        self.current_doc_path = None\n",
    "        self.current_doc_name = None\n",
    "\n",
    "    # ---------- IRIS-backed vector search helper (keeps the notebook's approach) ----------\n",
    "    def vector_search_iris(self, query: str, top_k: int = 5):\n",
    "        \"\"\"Query the IRIS VectorSearch.ORGstruct table and return (filename, content) results.\n",
    "        Returns empty list if no DB cursor is available.\n",
    "        \"\"\"\n",
    "        if self.cursor is None:\n",
    "            return []\n",
    "\n",
    "        search_vector = self.embedding_model.encode(\n",
    "            query,\n",
    "            normalize_embeddings=False,\n",
    "            show_progress_bar=False,\n",
    "        ).tolist()\n",
    "\n",
    "        # NOTE: we keep SQL structure used previously in the notebook.\n",
    "        search_sql = f\"\"\"\n",
    "            SELECT TOP {top_k} filename, content\n",
    "            FROM VectorSearch.ORGstruct\n",
    "            ORDER BY VECTOR_COSINE(vector, TO_VECTOR(?,DOUBLE)) DESC\n",
    "        \"\"\"\n",
    "        self.cursor.execute(search_sql, [str(search_vector)])\n",
    "        results = self.cursor.fetchall()\n",
    "        return results\n",
    "\n",
    "    # ---------- query routing via the orchestrator/agents ----------\n",
    "    def handle_query(self, query: str) -> str:\n",
    "        \"\"\"Use the orchestrator routing + agent invocation. This preserves the\n",
    "        RAG search + agent routing flow currently implemented in `Orchestrator`.\n",
    "        \"\"\"\n",
    "        return self.orchestrator.handle_query(query)\n",
    "\n",
    "\n",
    "# create a notebook-level adapter instance - reuses existing variables when available\n",
    "adapter = NotebookChatAdapter(globals().get('orchestrator'), globals().get('conn'), globals().get('cursor'), globals().get('model'))\n",
    "print(\"NotebookChatAdapter created as `adapter`. Use adapter.handle_query(<text>) or adapter.vector_search_iris(<text>)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747b7e70",
   "metadata": {},
   "source": [
    "### Interface with chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "55cfe6fc",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'RAGEngine' object has no attribute 'cursor'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      5\u001b[39m user_query = \u001b[33m\"\u001b[39m\u001b[33mJak je možné podat stížnost na dokumentaci v nemocnici?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Use the new adapter for queries\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43madapter\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_query\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 53\u001b[39m, in \u001b[36mNotebookChatAdapter.handle_query\u001b[39m\u001b[34m(self, query)\u001b[39m\n\u001b[32m     49\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mhandle_query\u001b[39m(\u001b[38;5;28mself\u001b[39m, query: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m     50\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Use the orchestrator routing + agent invocation. This preserves the\u001b[39;00m\n\u001b[32m     51\u001b[39m \u001b[33;03m    RAG search + agent routing flow currently implemented in `Orchestrator`.\u001b[39;00m\n\u001b[32m     52\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43morchestrator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Repo\\Codemedics_2\\Codemedics_test\\src\\chatbot_core\\orchestrator.py:79\u001b[39m, in \u001b[36mOrchestrator.handle_query\u001b[39m\u001b[34m(self, query)\u001b[39m\n\u001b[32m     78\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mhandle_query\u001b[39m(\u001b[38;5;28mself\u001b[39m, query: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m     results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrag\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvector_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     80\u001b[39m     rag_context = \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.join([\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mZ dokumentu \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfn\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m -> \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcontent\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m fn, content \u001b[38;5;129;01min\u001b[39;00m results])\n\u001b[32m     82\u001b[39m     agent_name = \u001b[38;5;28mself\u001b[39m.route_agent(query, rag_context)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Repo\\Codemedics_2\\Codemedics_test\\src\\chatbot_core\\rag.py:35\u001b[39m, in \u001b[36mRAGEngine.vector_search\u001b[39m\u001b[34m(self, user_prompt)\u001b[39m\n\u001b[32m     24\u001b[39m search_vector = \u001b[38;5;28mself\u001b[39m.embedding_model.encode(\n\u001b[32m     25\u001b[39m     user_prompt,\n\u001b[32m     26\u001b[39m     normalize_embeddings=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m     27\u001b[39m     show_progress_bar=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m     28\u001b[39m ).tolist()\n\u001b[32m     30\u001b[39m search_sql = \u001b[33m\"\"\"\u001b[39m\n\u001b[32m     31\u001b[39m \u001b[33m    SELECT TOP 5 filename, content \u001b[39m\n\u001b[32m     32\u001b[39m \u001b[33m    FROM VectorSearch.ORGstruct\u001b[39m\n\u001b[32m     33\u001b[39m \u001b[33m    ORDER BY VECTOR_COSINE(vector, TO_VECTOR(?,DOUBLE)) DESC\u001b[39m\n\u001b[32m     34\u001b[39m \u001b[33m\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcursor\u001b[49m.execute(search_sql, [\u001b[38;5;28mstr\u001b[39m(search_vector)])\n\u001b[32m     36\u001b[39m results = \u001b[38;5;28mself\u001b[39m.cursor.fetchall()\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mText z dokumentu \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m -> \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m x, y \u001b[38;5;129;01min\u001b[39;00m results]\n",
      "\u001b[31mAttributeError\u001b[39m: 'RAGEngine' object has no attribute 'cursor'"
     ]
    }
   ],
   "source": [
    "# Ensure orchestrator and adapter are present and demonstrate usage\n",
    "orchestrator = globals().get('orchestrator') or Orchestrator()\n",
    "adapter = globals().get('adapter') or NotebookChatAdapter(orchestrator, globals().get('conn'), globals().get('cursor'), globals().get('model'))\n",
    "\n",
    "user_query = \"Jak je možné podat stížnost na dokumentaci v nemocnici?\"\n",
    "# Use the new adapter for queries\n",
    "print(adapter.handle_query(user_query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc9e220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abych ti mohl přesně pomoct, potřebuju nejdřív zjistit, jaký konkrétní formulář se má pro takovou stížnost použít (např. „Formulář stížnosti zaměstnance“, „Podnět k nápravě“, „Incident – IT služby“ apod.).  \n",
      "\n",
      "V dostupné dokumentaci ale zatím nemám jednoznačný název formuláře pro „stížnost na nedostatečnou dokumentaci k interní aplikaci“.  \n",
      "Potřebuju proto jedno upřesnění:\n",
      "\n",
      "Používáte u vás v nemocnici nějaký z těchto typů formulářů (aspoň přibližně podle názvu)?\n",
      "1. „Stížnost / podnět zaměstnance“  \n",
      "2. „Formulář pro hlášení problému IT / IS“  \n",
      "3. „Formulář kvality – neshoda / podnět ke zlepšení“  \n",
      "\n",
      "Napiš prosím, ke kterému z těchto typů má tvoje stížnost nejblíž (nebo mi opiš název formuláře, který máte v interním systému), a pak tě začnu vést krok za krokem konkrétním formulářem.\n"
     ]
    }
   ],
   "source": [
    "# Example: use the adapter to answer a natural-language question\n",
    "sample_query = \"Chci si stěžovat na nedostatečnou dokumentaci k interní aplikaci — co mám udělat?\"\n",
    "print(adapter.handle_query(sample_query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e6d404",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
