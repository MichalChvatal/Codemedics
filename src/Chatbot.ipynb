{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c150491c",
   "metadata": {},
   "source": [
    "### Connect to server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11fbff3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import iris\n",
    "conn = iris.connect(\"localhost\", 32782, \"DEMO\", \"_SYSTEM\", \"ISCDEMO\") # Server, Port , Namespace, Username, Password\n",
    "cursor = conn.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99536ff2",
   "metadata": {},
   "source": [
    "### Make database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4269a401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to InterSystems IRIS\n"
     ]
    }
   ],
   "source": [
    "print(\"Connected to InterSystems IRIS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9484cb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "792a1157",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create database\n",
    "df = pd.read_json(\"./data/data.json\") #pd.DataFrame(out, columns=cols)  #replace with Tristans/Iaroslavs code\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4d783a",
   "metadata": {},
   "source": [
    "### Make encoding / add it to database\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ecf9ea11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4e45d0af7fd4fb79142d2f41d0a6f65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Make encoding \n",
    "#pip install sentence-transformers\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2') #might not work, if this is the case use instead 'all-MiniLM-L6-v2'\n",
    "\n",
    "embeddings = model.encode(df['content'].tolist(), normalize_embeddings=True, show_progress_bar=True)\n",
    "\n",
    "\n",
    "df['vector'] = embeddings.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cd0e3b4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "###Add to Iris:\n",
    "\n",
    "table_name = \"VectorSearch.ORGstruct\"\n",
    "\n",
    "create_table_query = f\"\"\"\n",
    "CREATE TABLE {table_name} (\n",
    "id INTEGER,\n",
    "filename LONGVARCHAR,\n",
    "content LONGVARCHAR,\n",
    "vector VECTOR(DOUBLE, 384)\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "cursor.execute(f\"DROP TABLE IF EXISTS {table_name}\" )\n",
    "cursor.execute(create_table_query)\n",
    "\n",
    "\n",
    "\n",
    "insert_query = f\"INSERT INTO {table_name} (id, filename, content, vector) values (?, ?, ?, TO_VECTOR(?))\"\n",
    "df[\"vector\"] = df[\"vector\"].astype(str)\n",
    "\n",
    "rows_list = df[[\"id\", \"filename\", \"content\", \"vector\"]].values.tolist()\n",
    "cursor.executemany(insert_query, rows_list)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66124774",
   "metadata": {},
   "source": [
    "### LLM setup (Ollama)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9b460849",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install ollama\n",
    "#pip install langchain langchain-ollama \n",
    "\n",
    "\n",
    "from ollama import chat\n",
    "from ollama import ChatResponse\n",
    "from langchain.agents import create_agent\n",
    "from langchain.agents.middleware import SummarizationMiddleware\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from langchain_ollama import ChatOllama\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38731d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import iris\n",
    "class RAGChatbot:\n",
    "    def __init__(self):\n",
    "        self.message_count = 0\n",
    "        conn = iris.connect(\"localhost\", 32782, \"DEMO\", \"_SYSTEM\", \"ISCDEMO\") # Server, Port , Namespace, Username, Password\n",
    "        self.cursor = conn.cursor()\n",
    "        self.agent = self.create_agent()\n",
    "        self.embedding_model = self.get_embedding_model()\n",
    "        \n",
    "        \n",
    "    def get_embedding_model(self):\n",
    "        return  SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        \n",
    "    def create_agent(self):\n",
    "        # Initialize model\n",
    "        llm = ChatOllama(model=\"gemma3:1b\") \n",
    "        \n",
    "        # Initialise short-term memory\n",
    "        checkpointer = InMemorySaver()\n",
    "        \n",
    "        # Create model\n",
    "        agent = create_agent(\n",
    "            model=llm, # Set model as our LLM \n",
    "            middleware=[\n",
    "                # create summarization proceedure - this creates summaries of our conversation to keep memory brief.\n",
    "                SummarizationMiddleware(\n",
    "                    model=llm,\n",
    "                    max_tokens_before_summary=4000,  # Trigger summarization at 4000 tokens\n",
    "                    messages_to_keep=20,  # Keep last 20 messages after summary\n",
    "                )\n",
    "            ],\n",
    "            # Creates the agent's memory with pre-initialized model\n",
    "            checkpointer=checkpointer,\n",
    "        )\n",
    "        self.config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "        return agent\n",
    "        \n",
    "    def vector_search(self, user_prompt): ###Bude potreba hodne upravit\n",
    "        search_vector =  self.embedding_model.encode(user_prompt, normalize_embeddings=False, show_progress_bar=False).tolist() \n",
    "        \n",
    "        search_sql = f\"\"\"\n",
    "            SELECT TOP 5 content \n",
    "            FROM VectorSearch.ORGstruct\n",
    "            ORDER BY VECTOR_COSINE(vector, TO_VECTOR(?,double)) DESC\n",
    "        \"\"\"\n",
    "        self.cursor.execute(search_sql,[str(search_vector)])\n",
    "        \n",
    "        results = self.cursor.fetchall()\n",
    "        return results\n",
    "\n",
    "    def get_prompt(self):\n",
    "       \n",
    "        query = input(\"\\n\\nHi, I'm a chatbot used for searching a patient's medical history. How can I help you today? \\n\\n - User: \")\n",
    "    \n",
    "        return query\n",
    "    \n",
    "    def validation(self, result):\n",
    "\n",
    "        return result\n",
    "    \n",
    "\n",
    "    def return_response(self):\n",
    "        query = self.get_prompt()\n",
    "\n",
    "        search = True\n",
    "        if self.message_count != 0:\n",
    "            search_ans = input(\"Search the database? [Y/N - default N]\")\n",
    "            if search_ans.lower() != \"y\":\n",
    "                search = False\n",
    "\n",
    "\n",
    "        if search:\n",
    " \n",
    "            results = self.vector_search(query)\n",
    "            if results == []:\n",
    "                print(\"No results found, check patient ID\")\n",
    "                return\n",
    "\n",
    "            prompt = f\"CONTEXT:\\n{results}\\n\\nUSER QUESTION:\\n{query}\"\n",
    "        else:\n",
    "            prompt = f\"USER QUESTION:\\n{query}\"\n",
    "\n",
    "        ##print(prompt)\n",
    "        system_prompt = \"You are a helpful and knowledgeable assistant designed to help a doctor interpret a patient's medical history using retrieved information from a database.\\\n",
    "        Please provide a detailed and medically relevant explanation, \\\n",
    "        include the dates of the information you are given.\"\n",
    "        response = self.agent.invoke({\"messages\" : [(\"system\", system_prompt), (\"user\", query), (\"system\", str(results))]}, self.config)\n",
    "        \n",
    "        response[\"messages\"][-1].pretty_print()\n",
    "        self.message_count += 1\n",
    "        \n",
    "        \n",
    "        validated_response = self.validation(response)\n",
    "\n",
    "        return validated_response \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747b7e70",
   "metadata": {},
   "source": [
    "### Interface with chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "55cfe6fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\AppData\\Local\\Temp\\ipykernel_23124\\1892484594.py:26: DeprecationWarning: max_tokens_before_summary is deprecated. Use trigger=('tokens', value) instead.\n",
      "  SummarizationMiddleware(\n",
      "C:\\Users\\Acer\\AppData\\Local\\Temp\\ipykernel_23124\\1892484594.py:26: DeprecationWarning: messages_to_keep is deprecated. Use keep=('messages', value) instead.\n",
      "  SummarizationMiddleware(\n"
     ]
    },
    {
     "ename": "ProgrammingError",
     "evalue": "<SQL ERROR>; Details: [SQLCODE: <-359>:<SQL Function (function stored procedure) not found>]\r\n[Location: <Prepare>]\r\n[%msg: < User defined SQL function 'SQLUSER.VECTOR_DOT' does not exist>]",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mProgrammingError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[58]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m bot = RAGChatbot()\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mbot\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreturn_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[57]\u001b[39m\u001b[32m, line 74\u001b[39m, in \u001b[36mRAGChatbot.return_response\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     69\u001b[39m         search = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     72\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m search:\n\u001b[32m---> \u001b[39m\u001b[32m74\u001b[39m     results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvector_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     75\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m results == []:\n\u001b[32m     76\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mNo results found, check patient ID\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[57]\u001b[39m\u001b[32m, line 46\u001b[39m, in \u001b[36mRAGChatbot.vector_search\u001b[39m\u001b[34m(self, user_prompt)\u001b[39m\n\u001b[32m     39\u001b[39m search_vector =  \u001b[38;5;28mself\u001b[39m.embedding_model.encode(user_prompt, normalize_embeddings=\u001b[38;5;28;01mFalse\u001b[39;00m, show_progress_bar=\u001b[38;5;28;01mFalse\u001b[39;00m).tolist() \n\u001b[32m     41\u001b[39m search_sql = \u001b[33mf\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m     42\u001b[39m \u001b[33m    SELECT TOP 5 content \u001b[39m\n\u001b[32m     43\u001b[39m \u001b[33m    FROM VectorSearch.ORGstruct\u001b[39m\n\u001b[32m     44\u001b[39m \u001b[33m    ORDER BY VECTOR_DOT(vector, TO_VECTOR(?,double)) DESC\u001b[39m\n\u001b[32m     45\u001b[39m \u001b[33m\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcursor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43msearch_sql\u001b[49m\u001b[43m,\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msearch_vector\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     48\u001b[39m results = \u001b[38;5;28mself\u001b[39m.cursor.fetchall()\n\u001b[32m     49\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
      "\u001b[31mProgrammingError\u001b[39m: <SQL ERROR>; Details: [SQLCODE: <-359>:<SQL Function (function stored procedure) not found>]\r\n[Location: <Prepare>]\r\n[%msg: < User defined SQL function 'SQLUSER.VECTOR_DOT' does not exist>]"
     ]
    }
   ],
   "source": [
    "bot = RAGChatbot()\n",
    "bot.return_response()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
