{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375d0a33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Repo\\Codemedics\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import iris\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.agents import create_agent\n",
    "from langchain.agents.middleware import SummarizationMiddleware\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from openai import OpenAI\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.agents import create_agent\n",
    "from langchain.agents.middleware import SummarizationMiddleware\n",
    "from langgraph.checkpoint.memory    import InMemorySaver\n",
    "from langchain_core.tools import tool\n",
    "from docx import Document\n",
    "import difflib\n",
    "import os\n",
    "import uuid\n",
    "from docx.oxml import OxmlElement   \n",
    "\n",
    "from chatbot_core.orchestrator import Orchestrator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c150491c",
   "metadata": {},
   "source": [
    "### Connect to server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11fbff3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "conn = iris.connect(\"localhost\", 32782, \"DEMO\", \"_SYSTEM\", \"ISCDEMO\") # Server, Port , Namespace, Username, Password\n",
    "cursor = conn.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99536ff2",
   "metadata": {},
   "source": [
    "### Make database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4269a401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to InterSystems IRIS\n"
     ]
    }
   ],
   "source": [
    "print(\"Connected to InterSystems IRIS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792a1157",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create database\n",
    "df = pd.read_json(\"./data/data.json\") #pd.DataFrame(out, columns=cols)  #replace with Tristans/Iaroslavs code\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4d783a",
   "metadata": {},
   "source": [
    "### Make encoding / add it to database\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf9ea11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 5/5 [00:06<00:00,  1.33s/it]\n",
      "Batches: 100%|██████████| 5/5 [00:06<00:00,  1.33s/it]\n"
     ]
    }
   ],
   "source": [
    "#Make encoding \n",
    "#pip install sentence-transformers\n",
    "\n",
    "model = SentenceTransformer('pritamdeka/S-PubMedBert-MS-MARCO') #might not work, if this is the case use instead 'all-MiniLM-L6-v2'\n",
    "\n",
    "\n",
    "def vectorize_content(df):\n",
    "    embeddings = model.encode(df['content'].tolist(), normalize_embeddings=True, show_progress_bar=True)\n",
    "    return embeddings\n",
    "\n",
    "def vectorize_filename(df):\n",
    "    embeddings = model.encode(df['filename'].tolist(), normalize_embeddings=True, show_progress_bar=True)\n",
    "    return embeddings\n",
    "\n",
    "embeddings = vectorize_content(df)\n",
    "df['vector'] = embeddings.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0e3b4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "###Add to Iris:\n",
    "def create_table(df):\n",
    "    table_name = \"VectorSearch.ORGstruct\"\n",
    "\n",
    "    create_table_query = f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {table_name} (\n",
    "    id INTEGER,\n",
    "    filename LONGVARCHAR,\n",
    "    content LONGVARCHAR,\n",
    "    vector VECTOR(DOUBLE, 786)\n",
    "    )\n",
    "    \"\"\"\n",
    "\n",
    "    cursor.execute(create_table_query)\n",
    "\n",
    "\n",
    "\n",
    "    insert_query = f\"INSERT INTO {table_name} (id, filename, content, vector) values (?, ?, ?, TO_VECTOR(?))\"\n",
    "    df[\"vector\"] = df[\"vector\"].astype(str)\n",
    "\n",
    "    rows_list = df[[\"id\", \"filename\", \"content\", \"vector\"]].values.tolist()\n",
    "    cursor.executemany(insert_query, rows_list)\n",
    "    print(\"Done\")\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "create_table(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66124774",
   "metadata": {},
   "source": [
    "### LLM setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4158ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "  api_key=\"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\"\n",
    ")\n",
    "OPENAI_API_KEY = \"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\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66246a7",
   "metadata": {},
   "source": [
    "### OpenAI SDK compatibility\n",
    "\n",
    "The python-openai package changed its API in recent major releases. If you see errors like `APIRemovedInV1` or `ChatCompletion.create` failures, you have two options:\n",
    "\n",
    "- Pin to the older client (backwards compatible): `pip install openai==0.28` — this keeps support for `openai.ChatCompletion.create`.\n",
    "- Prefer the new client API: create an `OpenAI` client and use the `client.chat.completions.create(...)` or `client.chat.create(...)` methods. The notebook and `agents` module handle both styles and also support injecting a custom `llm_client` for demos and testing.\n",
    "\n",
    "If you're running the notebook interactively, it's safest to set your `OPENAI_API_KEY` as an environment variable rather than hard-coding it in the notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16bbcd53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NotebookChatAdapter created as `adapter`. Use adapter.handle_query(<text>) or adapter.vector_search_iris(<text>)\n"
     ]
    }
   ],
   "source": [
    "# Adapter to use the new Orchestrator + keep IRIS / vector helpers\n",
    "# This replaces the old large RAGChatbot class with a small adapter that\n",
    "# delegates to chatbot_core.Orchestrator and still exposes the IRIS-based\n",
    "# vector search and the document helper functionality that the notebook\n",
    "# earlier implemented.\n",
    "\n",
    "class NotebookChatAdapter:\n",
    "    def __init__(self, orchestrator=None, conn=None, cursor=None, embedding_model=None):\n",
    "        # orchestrator (chatbot_core.orchestrator.Orchestrator) is preferred\n",
    "        self.orchestrator = orchestrator or Orchestrator()\n",
    "\n",
    "        # keep existing IRIS connection and DB cursor if available\n",
    "        self.conn = conn\n",
    "        self.cursor = cursor\n",
    "\n",
    "        # reuse notebook-level embedding model (variable `model`) if present\n",
    "        self.embedding_model = embedding_model or globals().get(\"model\") or SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "        # document state (mirrors earlier notebook behaviour)\n",
    "        self.current_doc = None\n",
    "        self.current_doc_path = None\n",
    "        self.current_doc_name = None\n",
    "\n",
    "    # ---------- IRIS-backed vector search helper (keeps the notebook's approach) ----------\n",
    "    def vector_search_iris(self, query: str, top_k: int = 5):\n",
    "        \"\"\"Query the IRIS VectorSearch.ORGstruct table and return (filename, content) results.\n",
    "        Returns empty list if no DB cursor is available.\n",
    "        \"\"\"\n",
    "        if self.cursor is None:\n",
    "            return []\n",
    "\n",
    "        search_vector = self.embedding_model.encode(\n",
    "            query,\n",
    "            normalize_embeddings=False,\n",
    "            show_progress_bar=False,\n",
    "        ).tolist()\n",
    "\n",
    "        # NOTE: we keep SQL structure used previously in the notebook.\n",
    "        search_sql = f\"\"\"\n",
    "            SELECT TOP {top_k} filename, content\n",
    "            FROM VectorSearch.ORGstruct\n",
    "            ORDER BY VECTOR_COSINE(vector, TO_VECTOR(?,DOUBLE)) DESC\n",
    "        \"\"\"\n",
    "        self.cursor.execute(search_sql, [str(search_vector)])\n",
    "        results = self.cursor.fetchall()\n",
    "        return results\n",
    "\n",
    "    # ---------- query routing via the orchestrator/agents ----------\n",
    "    def handle_query(self, query: str) -> str:\n",
    "        \"\"\"Use the orchestrator routing + agent invocation. This preserves the\n",
    "        RAG search + agent routing flow currently implemented in `Orchestrator`.\n",
    "        \"\"\"\n",
    "        return self.orchestrator.handle_query(query)\n",
    "\n",
    "\n",
    "# create a notebook-level adapter instance - reuses existing variables when available\n",
    "adapter = NotebookChatAdapter(globals().get('orchestrator'), globals().get('conn'), globals().get('cursor'), globals().get('model'))\n",
    "print(\"NotebookChatAdapter created as `adapter`. Use adapter.handle_query(<text>) or adapter.vector_search_iris(<text>)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747b7e70",
   "metadata": {},
   "source": [
    "### Interface with chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "55cfe6fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abych ti mohl přesně poradit, potřebuju nejdřív upřesnit jednu věc:\n",
      "\n",
      "Máš na mysli:\n",
      "1) obecně „jak podat stížnost“ (bez konkrétního formuláře), nebo  \n",
      "2) chceš krok za krokem vyplnit konkrétní nemocniční formulář „Stížnost na zdravotnickou dokumentaci / dokumentaci péče“?\n",
      "\n",
      "Napiš prosím jen číslo 1 nebo 2 (případně název formuláře, pokud ho znáš z nemocnice).\n"
     ]
    }
   ],
   "source": [
    "# Ensure orchestrator and adapter are present and demonstrate usage\n",
    "orchestrator = globals().get('orchestrator') or Orchestrator()\n",
    "adapter = globals().get('adapter') or NotebookChatAdapter(orchestrator, globals().get('conn'), globals().get('cursor'), globals().get('model'))\n",
    "\n",
    "user_query = \"Jak je možné podat stížnost na dokumentaci v nemocnici?\"\n",
    "# Use the new adapter for queries\n",
    "print(adapter.handle_query(user_query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4fc9e220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abych ti mohl přesně pomoct, potřebuju nejdřív zjistit, jaký konkrétní formulář se má pro takovou stížnost použít (např. „Formulář stížnosti zaměstnance“, „Podnět k nápravě“, „Incident – IT služby“ apod.).  \n",
      "\n",
      "V dostupné dokumentaci ale zatím nemám jednoznačný název formuláře pro „stížnost na nedostatečnou dokumentaci k interní aplikaci“.  \n",
      "Potřebuju proto jedno upřesnění:\n",
      "\n",
      "Používáte u vás v nemocnici nějaký z těchto typů formulářů (aspoň přibližně podle názvu)?\n",
      "1. „Stížnost / podnět zaměstnance“  \n",
      "2. „Formulář pro hlášení problému IT / IS“  \n",
      "3. „Formulář kvality – neshoda / podnět ke zlepšení“  \n",
      "\n",
      "Napiš prosím, ke kterému z těchto typů má tvoje stížnost nejblíž (nebo mi opiš název formuláře, který máte v interním systému), a pak tě začnu vést krok za krokem konkrétním formulářem.\n"
     ]
    }
   ],
   "source": [
    "# Example: use the adapter to answer a natural-language question\n",
    "sample_query = \"Chci si stěžovat na nedostatečnou dokumentaci k interní aplikaci — co mám udělat?\"\n",
    "print(adapter.handle_query(sample_query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e6d404",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
