{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "375d0a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import iris\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.agents import create_agent\n",
    "from langchain.agents.middleware import SummarizationMiddleware\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from langchain_ollama import ChatOllama\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c150491c",
   "metadata": {},
   "source": [
    "### Connect to server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11fbff3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "conn = iris.connect(\"localhost\", 32782, \"DEMO\", \"_SYSTEM\", \"ISCDEMO\") # Server, Port , Namespace, Username, Password\n",
    "cursor = conn.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99536ff2",
   "metadata": {},
   "source": [
    "### Make database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4269a401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to InterSystems IRIS\n"
     ]
    }
   ],
   "source": [
    "print(\"Connected to InterSystems IRIS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "792a1157",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create database\n",
    "df = pd.read_json(\"./data/data.json\") #pd.DataFrame(out, columns=cols)  #replace with Tristans/Iaroslavs code\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4d783a",
   "metadata": {},
   "source": [
    "### Make encoding / add it to database\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ecf9ea11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccc7486f1438440191a96bd366342078",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Make encoding \n",
    "#pip install sentence-transformers\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2') #might not work, if this is the case use instead 'all-MiniLM-L6-v2'\n",
    "\n",
    "embeddings = model.encode(df['content'].tolist(), normalize_embeddings=True, show_progress_bar=True)\n",
    "\n",
    "\n",
    "df['vector'] = embeddings.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd0e3b4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "###Add to Iris:\n",
    "def create_table(df):\n",
    "    table_name = \"VectorSearch.ORGstruct\"\n",
    "\n",
    "    create_table_query = f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {table_name} (\n",
    "    id INTEGER,\n",
    "    filename LONGVARCHAR,\n",
    "    content LONGVARCHAR,\n",
    "    vector VECTOR(DOUBLE, 384)\n",
    "    )\n",
    "    \"\"\"\n",
    "\n",
    "    cursor.execute(f\"DROP TABLE IF EXISTS {table_name}\" )\n",
    "    cursor.execute(create_table_query)\n",
    "\n",
    "\n",
    "\n",
    "    insert_query = f\"INSERT INTO {table_name} (id, filename, content, vector) values (?, ?, ?, TO_VECTOR(?))\"\n",
    "    df[\"vector\"] = df[\"vector\"].astype(str)\n",
    "\n",
    "    rows_list = df[[\"id\", \"filename\", \"content\", \"vector\"]].values.tolist()\n",
    "    cursor.executemany(insert_query, rows_list)\n",
    "    print(\"Done\")\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "create_table(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66124774",
   "metadata": {},
   "source": [
    "### LLM setup (Ollama)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16bbcd53",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGChatbot:\n",
    "    def __init__(self):\n",
    "        self.message_count = 0\n",
    "        conn = iris.connect(\"localhost\", 32782, \"DEMO\", \"_SYSTEM\", \"ISCDEMO\") # Server, Port , Namespace, Username, Password\n",
    "        self.cursor = conn.cursor()\n",
    "        self.agent = self.create_agent()\n",
    "        self.embedding_model = self.get_embedding_model()\n",
    "        \n",
    "        \n",
    "    def get_embedding_model(self):\n",
    "        return  SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        \n",
    "    def create_agent(self):\n",
    "        # Initialize model\n",
    "        llm = ChatOllama(model=\"gemma3:1b\") \n",
    "        \n",
    "        # Initialise short-term memory\n",
    "        checkpointer = InMemorySaver()\n",
    "        \n",
    "        # Create model\n",
    "        agent = create_agent(\n",
    "            model=llm, # Set model as our LLM \n",
    "            middleware=[\n",
    "                # create summarization proceedure - this creates summaries of our conversation to keep memory brief.\n",
    "                SummarizationMiddleware(\n",
    "                    model=llm,\n",
    "                    trigger=('tokens', 4000),  # Trigger summarization at 4000 tokens\n",
    "                    keep=('messages', 20),  # Keep last 20 messages after summary\n",
    "                )\n",
    "            ],\n",
    "            # Creates the agent's memory with pre-initialized model\n",
    "            checkpointer=checkpointer,\n",
    "        )\n",
    "        self.config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "        return agent\n",
    "        \n",
    "    def vector_search(self, user_prompt): \n",
    "        search_vector =  self.embedding_model.encode(user_prompt, normalize_embeddings=False, show_progress_bar=False).tolist() \n",
    "        \n",
    "        search_sql = f\"\"\"\n",
    "            SELECT TOP 5 filename, content \n",
    "            FROM VectorSearch.ORGstruct\n",
    "            ORDER BY VECTOR_COSINE(vector, TO_VECTOR(?,DOUBLE)) DESC\n",
    "        \"\"\"\n",
    "\n",
    "        self.cursor.execute(search_sql, [str(search_vector)])\n",
    "        \n",
    "\n",
    "        results = self.cursor.fetchall()\n",
    "\n",
    "        results = [f\"Text z dokumentu {x} -> {y}\" for x, y in results]\n",
    "\n",
    "\n",
    "        return results\n",
    "\n",
    "    def get_prompt(self):\n",
    "       \n",
    "        query = input(\"\\n\\nHi, I'm a chatbot used for searching a patient's medical history. How can I help you today? \\n\\n - User: \")\n",
    "    \n",
    "        return query\n",
    "    \n",
    "    def validation(self, result):\n",
    "\n",
    "        return result\n",
    "    \n",
    "\n",
    "    def return_response(self):\n",
    "        query = self.get_prompt()\n",
    "\n",
    "        search = True\n",
    "        if self.message_count != 0:\n",
    "            search_ans = input(\"Search the database? [Y/N - default N]\")\n",
    "            if search_ans.lower() != \"y\":\n",
    "                search = False\n",
    "\n",
    "\n",
    "        if search:\n",
    " \n",
    "            results = self.vector_search(query)\n",
    " \n",
    "            if results == []:\n",
    "                print(\"No results found, check patient ID\")\n",
    "                return\n",
    "\n",
    "            prompt = f\"CONTEXT:\\n{results}\\n\\nUSER QUESTION:\\n{query}\"\n",
    "        else:\n",
    "            prompt = f\"USER QUESTION:\\n{query}\"\n",
    "\n",
    "        ##print(prompt)\n",
    "        system_prompt = \"\"\"\n",
    "- Jsi užitečný asistent, chatbot fungující v nemocnici.\n",
    "- Tvým posláním je odpovídat na dotazy zaměstnanců týkající se jejich práce a provádět je organizační strukturou nemocnice a administrativními procesy.\n",
    "- Při odpovídání se vždy řiď těmito pravidly:\n",
    "    1. Nelži a nevymýšlej si fakta\n",
    "    2. Vyhýbej se nepodloženému nebo podlézavému lichocení\n",
    "    3. Zachovej profesionalitu a střízlivou upřímnost\n",
    "    4. Odpovídej v jazyku, jakým mluví uživatel.\n",
    "- Když se uživatel dotazuje na nějaký proces v nemocnici (např. \\\"Chci si stěžovat na nedostatečnou dokumentaci k webové aplikaci vyvinuté v Centru Informatiky (CI)\\\"),\n",
    "  1. Odpovídej jasně a požádej uživatele o upřesnění, pokud nemůžeš přesně určit proces, který je pro uživatele relevantní (v tomto případě proces dokumentace ze strany oddělení nezdravotnických aplikací, které je součástí CI).\n",
    "  2. Pokud má uživatel podle předpisů více možností, jak dosáhnout svého cíle, popiš dostupné možnosti a zeptej se uživatele, kterou si chce vybrat.\n",
    "    - Pokud musí kontaktovat jiného zaměstnance, ale nemáš jeho kontaktní údaje, *jasně mu sděl, že je nemáš*.\n",
    "    - Pokud musí kontaktovat jiného zaměstnance a ty máš jeho kontaktní údaje, poskytni mu tyto informace (jméno, telefonní číslo, e-mail).\n",
    "  3. Při odpovídání *vždy* upřednostňuj organizační informace z dodaných dokumentů. Pokud tam informace není dostupná, *informuj o tom uživatele* a nic si nevymýšlej.\n",
    "  4. Pokud nemáš informace o uživatelově dotazu nebo o tom, jak by měl uživatel v daném procesu postupovat, ale *máš* informace o tom, kde může uživatel získat kvalifikovanou pomoc, doporuč mu osoby, které má kontaktovat, a poskytni kontaktní informace (V tomto případě by měl uživatel kontaktovat oddělení nezdravotnických aplikací).\n",
    "  5. Na konci své odpovědi *vždy* odkazuj k dokumentům (text \"Z dokumentu XYZ.docx\", před ->), ze kterých jsi čerpal informace. Vypiš je na konci odpovědi ve formátu: \"Dále se můžete obrátit na dokument XYZ\".\n",
    "  6. Pokud uživatel poprosí o pomoc s procesem, proveď ho tím několika kroky, které musí podniknout, aby dosáhl svého cíle.\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        response = self.agent.invoke({\"messages\" : [(\"system\", system_prompt), (\"user\", query), (\"system\", str(results))]}, self.config)\n",
    "        \n",
    "        print(results)\n",
    "        self.message_count += 1\n",
    "        \n",
    "        validated_response = self.validation(response)\n",
    "\n",
    "        return validated_response[\"messages\"][-1].pretty_print()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747b7e70",
   "metadata": {},
   "source": [
    "### Interface with chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "55cfe6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "bot = RAGChatbot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4fc9e220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Text z dokumentu Organizační ...ditu a kontroly.docx -> 4.4. Pracovní náplň a pravomoci zaměstnanců OIAK 3', 'Text z dokumentu Organizační ...nukleární medicíny.doc -> 5. Odpovědnosti a pravomoci 13', 'Text z dokumentu Organizační ...Kanceláře ředitele.doc -> 5. Odpovědnosti a pravomoci 9', 'Text z dokumentu Organizační ...ditu a kontroly.docx -> 4.3. Vnitřní komunikace 3', 'Text z dokumentu Organizační ... vědu a výzkum.doc -> 5. Odpovědnosti a pravomoci 7']\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Oddělení nezdravotnických aplikací.\n"
     ]
    }
   ],
   "source": [
    "bot.return_response()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
