{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c150491c",
   "metadata": {},
   "source": [
    "### Connect to server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11fbff3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import iris\n",
    "conn = iris.connect(\"localhost\", 32782, \"DEMO\", \"_SYSTEM\", \"ISCDEMO\") # Server, Port , Namespace, Username, Password\n",
    "cursor = conn.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99536ff2",
   "metadata": {},
   "source": [
    "### Make database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792a1157",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create database\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4d783a",
   "metadata": {},
   "source": [
    "### Make encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf9ea11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make encoding \n",
    "#pip install sentence-transformers\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('pritamdeka/S-PubMedBert-MS-MARCO') #might not work, if this is the case use instead 'all-MiniLM-L6-v2'\n",
    "\n",
    "embeddings = model.encode(df['data'].tolist(), normalize_embeddings=True) #proably want to edit this so we encode only some parts of data - names not relevant.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a8d705",
   "metadata": {},
   "source": [
    "### Add encoding to database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a6d124",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add encoding to the database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66124774",
   "metadata": {},
   "source": [
    "### LLM setup (Ollama)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b460849",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install ollama\n",
    "#pip install langchain langchain-ollama \n",
    "\n",
    "\n",
    "from ollama import chat\n",
    "from ollama import ChatResponse\n",
    "from langchain.agents import create_agent\n",
    "from langchain.agents.middleware import SummarizationMiddleware\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from langchain_ollama import ChatOllama\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38731d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import iris\n",
    "class RAGChatbot:\n",
    "    def __init__(self):\n",
    "        self.message_count = 0\n",
    "        conn = iris.connect(\"localhost\", 32782, \"DEMO\", \"_SYSTEM\", \"ISCDEMO\") # Server, Port , Namespace, Username, Password\n",
    "        self.cursor = conn.cursor()\n",
    "        self.agent = self.create_agent()\n",
    "        self.embedding_model = self.get_embedding_model()\n",
    "        \n",
    "        \n",
    "    def get_embedding_model(self):\n",
    "        return  SentenceTransformer('pritamdeka/S-PubMedBert-MS-MARCO')\n",
    "        \n",
    "    def create_agent(self):\n",
    "        # Initialize model\n",
    "        llm = ChatOllama(model=\"gemma3:1b\") \n",
    "        \n",
    "        # Initialise short-term memory\n",
    "        checkpointer = InMemorySaver()\n",
    "        \n",
    "        # Create model\n",
    "        agent = create_agent(\n",
    "            model=llm, # Set model as our LLM \n",
    "            middleware=[\n",
    "                # create summarization proceedure - this creates summaries of our conversation to keep memory brief.\n",
    "                SummarizationMiddleware(\n",
    "                    model=llm,\n",
    "                    max_tokens_before_summary=4000,  # Trigger summarization at 4000 tokens\n",
    "                    messages_to_keep=20,  # Keep last 20 messages after summary\n",
    "                )\n",
    "            ],\n",
    "            # Creates the agent's memory with pre-initialized model\n",
    "            checkpointer=checkpointer,\n",
    "        )\n",
    "        self.config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "        return agent\n",
    "        \n",
    "    def vector_search(self, user_prompt): ###Bude potreba hodne upravit\n",
    "        search_vector =  self.embedding_model.encode(user_prompt, normalize_embeddings=True, show_progress_bar=False).tolist() \n",
    "        \n",
    "        search_sql = f\"\"\"\n",
    "            SELECT TOP 5 ClinicalNotes \n",
    "            FROM ZmÄ›n VectorSearch.DocRefVectors\n",
    "            ORDER BY VECTOR_DISTANCE(NotesVector, TO_VECTOR(?,double)) DESC\n",
    "        \"\"\"\n",
    "        self.cursor.execute(search_sql,[str(search_vector)])\n",
    "        \n",
    "        results = self.cursor.fetchall()\n",
    "        return results\n",
    "\n",
    "    def get_prompt(self):\n",
    "       \n",
    "        query = input(\"\\n\\nHi, I'm a chatbot used for searching a patient's medical history. How can I help you today? \\n\\n - User: \")\n",
    "    \n",
    "        return query\n",
    "    \n",
    "    def validation(self, result):\n",
    "\n",
    "        return result\n",
    "    \n",
    "\n",
    "    def return_response(self):\n",
    "        query = self.get_prompt()\n",
    "\n",
    "        search = True\n",
    "        if self.message_count != 0:\n",
    "            search_ans = input(\"Search the database? [Y/N - default N]\")\n",
    "            if search_ans.lower() != \"y\":\n",
    "                search = False\n",
    "\n",
    "\n",
    "        if search:\n",
    "            try:\n",
    "                patient_id = int(input(\"What is the patient ID?\"))\n",
    "            except:\n",
    "                print(\"The patient ID should be an integer\")\n",
    "                return\n",
    "\n",
    "            results = self.vector_search(query, patient_id)\n",
    "            if results == []:\n",
    "                print(\"No results found, check patient ID\")\n",
    "                return\n",
    "\n",
    "            prompt = f\"CONTEXT:\\n{results}\\n\\nUSER QUESTION:\\n{query}\"\n",
    "        else:\n",
    "            prompt = f\"USER QUESTION:\\n{query}\"\n",
    "\n",
    "        ##print(prompt)\n",
    "        system_prompt = \"You are a helpful and knowledgeable assistant designed to help a doctor interpret a patient's medical history using retrieved information from a database.\\\n",
    "        Please provide a detailed and medically relevant explanation, \\\n",
    "        include the dates of the information you are given.\"\n",
    "        response = self.agent.invoke({\"messages\" : [(\"system\", system_prompt), (\"user\", query), (\"system\", str(results))]}, self.config)\n",
    "        \n",
    "        response[\"messages\"][-1].pretty_print()\n",
    "        self.message_count += 1\n",
    "        \n",
    "        \n",
    "        validated_response = self.validation(response)\n",
    "\n",
    "        return validated_response \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747b7e70",
   "metadata": {},
   "source": [
    "### Interface with chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55cfe6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "bot = RAGChatbot()\n",
    "bot.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
